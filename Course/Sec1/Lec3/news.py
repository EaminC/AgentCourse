"""
ç™¾åº¦æ–°é—» RSS çˆ¬è™« - ä¿å­˜æ ‡é¢˜ã€é“¾æ¥å’ŒäºŒçº§é¡µé¢ HTML
å°è¯•æŠ“æ­£æ–‡ï¼Œå¦‚æœæŠ“ä¸åˆ°ï¼Œè‡³å°‘ä¿å­˜ HTML
"""

import feedparser
from bs4 import BeautifulSoup
import os
import requests

# RSS é“¾æ¥
rss_url = "http://news.baidu.com/n?cmd=4&class=civilnews&tn=rss"
feed = feedparser.parse(rss_url)

print(f"ğŸ—ï¸ æ‰¾åˆ° {len(feed.entries)} æ¡æ–°é—»\n")

# åˆ›å»ºä¿å­˜æ–‡ä»¶å¤¹
data_dir = "data"
if not os.path.exists(data_dir):
    os.makedirs(data_dir)

# æ¸…ç†æ–‡ä»¶åå‡½æ•°
def clean_filename(title):
    filename = title
    for char in [":", "/", "\\", "?", "*", "<", ">", "|", '"']:
        filename = filename.replace(char, "")
    filename = filename.strip()
    if len(filename) > 50:
        filename = filename[:50]
    return filename

news_count = 0

for entry in feed.entries[:100]:
    title = entry.title
    link = entry.link
    print("ğŸ“° æ ‡é¢˜:", title)
    print("ğŸ”— é“¾æ¥:", link)
    
    full_text = ""
    html_content = ""
    try:
        # ä¸‹è½½äºŒçº§é¡µé¢ HTML
        response = requests.get(link, timeout=10)
        response.encoding = response.apparent_encoding
        html_content = response.text
        
        # å°è¯•è§£ææ­£æ–‡
        soup = BeautifulSoup(html_content, "html.parser")
        
        # ç™¾å®¶å·æ­£æ–‡é€šå¸¸åœ¨ class åŒ…å« "article" æˆ– "article-content" çš„ div å†…
        article_div = soup.find("div", class_="article") or soup.find("div", class_="article-content")
        if article_div:
            paragraphs = article_div.find_all("p")
            text_list = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 10]
            full_text = "\n\n".join(text_list)
    except Exception as e:
        print(f"âŒ ä¸‹è½½æˆ–è§£æå¤±è´¥: {e}")
    
    # å¦‚æœæ­£æ–‡ä¸ºç©ºï¼Œæç¤ºç”¨æˆ·
    if len(full_text) < 30:
        full_text = "âš ï¸ æ­£æ–‡æŠ“å–å¤±è´¥æˆ–ä¸ºç©ºï¼Œå¯æ‰“å¼€å¯¹åº” HTML æŸ¥çœ‹\n"
    
    # ä¿å­˜æ–‡æœ¬ä¿¡æ¯
    filename_base = clean_filename(title)
    txt_filepath = os.path.join(data_dir, filename_base + ".txt")
    try:
        with open(txt_filepath, "w", encoding="utf-8") as f:
            f.write("æ ‡é¢˜: " + title + "\n")
            f.write("é“¾æ¥: " + link + "\n")
            f.write("\n" + "="*50 + "\n\n")
            f.write(full_text)
        print(f"âœ… å·²ä¿å­˜æ–‡æœ¬: {txt_filepath}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡æœ¬å¤±è´¥: {e}")
    
    # å¦å¤–ä¿å­˜åŸå§‹ HTMLï¼Œæ–¹ä¾¿ä¹‹åæ‰‹åŠ¨æŸ¥çœ‹æˆ–é‡æ–°è§£æ
    if html_content:
        html_filepath = os.path.join(data_dir, filename_base + ".html")
        try:
            with open(html_filepath, "w", encoding="utf-8") as f_html:
                f_html.write(html_content)
            print(f"âœ… å·²ä¿å­˜HTML: {html_filepath}")
        except Exception as e:
            print(f"âŒ ä¿å­˜HTMLå¤±è´¥: {e}")
    
    news_count += 1
    print()

print(f"\nğŸ‰ å®Œæˆï¼å…±ä¿å­˜äº† {news_count} æ¡æ–°é—»")
