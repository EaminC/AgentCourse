"""
æ–°é—»çˆ¬è™« - Python åˆå­¦è€…å…¥é—¨æ¡ˆä¾‹ï¼ˆé¢å‘å¯¹è±¡ç‰ˆæœ¬ï¼‰

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†ä»¥ä¸‹ Python åŸºç¡€æ¦‚å¿µï¼š
1. å¯¼å…¥æ¨¡å— (import)
2. å˜é‡å’Œæ•°æ®ç±»å‹
3. å­—ç¬¦ä¸²æ“ä½œ
4. åˆ—è¡¨ (list) å’Œå¾ªç¯ (for)
5. æ¡ä»¶åˆ¤æ–­ (if/else)
6. å¼‚å¸¸å¤„ç† (try/except)
7. æ–‡ä»¶æ“ä½œ
8. å‡½æ•°å®šä¹‰
9. é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼ˆç±»ã€å¯¹è±¡ã€æ–¹æ³•ï¼‰
"""

# ========== ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥æ¨¡å— ==========
import feedparser      # è§£æ RSS è®¢é˜…æº
from bs4 import BeautifulSoup  # è§£æ HTML
import os              # æ–‡ä»¶ç³»ç»Ÿæ“ä½œ
import requests        # å‘é€ HTTP è¯·æ±‚


# ========== ç¬¬äºŒæ­¥ï¼šå®šä¹‰ NewsCrawler ç±» ==========
class NewsCrawler:
    """æ–°é—»çˆ¬è™«ç±»"""
    
    def __init__(self, rss_url, data_dir="data"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param rss_url: RSS è®¢é˜…æºåœ°å€
        :param data_dir: æ•°æ®ä¿å­˜ç›®å½•
        """
        self.rss_url = rss_url
        self.data_dir = data_dir
        self.news_count = 0
        
        # åˆ›å»ºä¿å­˜æ–‡ä»¶å¤¹
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
    
    def clean_filename(self, title):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
        filename = title
        for char in [":", "/", "\\", "?", "*", "<", ">", "|", '"']:
            filename = filename.replace(char, "")
        filename = filename.strip()
        if len(filename) > 50:
            filename = filename[:50]
        return filename
    
    def download_article(self, link):
        """
        ä¸‹è½½å¹¶è§£ææ–°é—»æ­£æ–‡
        :param link: æ–°é—»é“¾æ¥
        :return: æ–°é—»æ­£æ–‡å†…å®¹ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            # ä¸‹è½½ç½‘é¡µ
            response = requests.get(link, timeout=10)
            response.encoding = response.apparent_encoding
            
            # è§£æ HTML
            soup = BeautifulSoup(response.text, "html.parser")
            
            # æå–æ‰€æœ‰æ®µè½
            paragraphs = soup.find_all("p")
            text_list = []
            for p in paragraphs:
                text = p.get_text(strip=True)
                if len(text) > 30:  # åªä¿ç•™è¾ƒé•¿çš„æ®µè½
                    text_list.append(text)
            
            return "\n\n".join(text_list)
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}\n")
            return ""
    
    def save_article(self, title, link, content):
        """
        ä¿å­˜æ–°é—»åˆ°æ–‡ä»¶
        :param title: æ–°é—»æ ‡é¢˜
        :param link: æ–°é—»é“¾æ¥
        :param content: æ–°é—»å†…å®¹
        """
        filename = self.clean_filename(title) + ".txt"
        filepath = os.path.join(self.data_dir, filename)
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write("æ ‡é¢˜: " + title + "\n")
            f.write("é“¾æ¥: " + link + "\n")
            f.write("\n" + "=" * 50 + "\n\n")
            f.write(content)
        
        print(f"ğŸ’¾ å·²ä¿å­˜: {filename}\n")
        self.news_count += 1
    
    def run(self, max_news=10):
        """
        è¿è¡Œçˆ¬è™«ä¸»æµç¨‹
        :param max_news: æœ€å¤šçˆ¬å–å¤šå°‘æ¡æ–°é—»
        """
        # è§£æ RSS è®¢é˜…æº
        feed = feedparser.parse(self.rss_url)
        print(f"ğŸ—ï¸ æ‰¾åˆ° {len(feed.entries)} æ¡æ–°é—»\n")
        
        # éå†æ–°é—»å¹¶ä¿å­˜
        for entry in feed.entries[:max_news]:
            title = entry.title
            link = entry.link
            
            print("ğŸ“° æ ‡é¢˜:", title)
            
            # ä¸‹è½½æ–°é—»æ­£æ–‡
            full_text = self.download_article(link)
            
            # å¦‚æœæ­£æ–‡ä¸ºç©ºï¼Œè·³è¿‡
            if len(full_text) < 50:
                print("âš ï¸  æ­£æ–‡ä¸ºç©ºï¼Œè·³è¿‡\n")
                continue
            
            print(f"ğŸ“„ æ­£æ–‡: {len(full_text)} å­—\n")
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            self.save_article(title, link, full_text)
        
        print(f"âœ… å®Œæˆï¼å…±ä¿å­˜ {self.news_count} æ¡æ–°é—»")


# ========== ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ç±»åˆ›å»ºå¯¹è±¡å¹¶è¿è¡Œ ==========
if __name__ == "__main__":
    # åˆ›å»ºçˆ¬è™«å¯¹è±¡
    rss_url = "http://news.baidu.com/n?cmd=4&class=civilnews&tn=rss"
    crawler = NewsCrawler(rss_url, data_dir="data")
    
    # è¿è¡Œçˆ¬è™«
    crawler.run(max_news=10)

