"""
æ–°é—»çˆ¬è™« - Python åˆå­¦è€…å…¥é—¨æ¡ˆä¾‹ï¼ˆé¢å‘å¯¹è±¡ç‰ˆæœ¬ï¼‰

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†ä»¥ä¸‹ Python åŸºç¡€æ¦‚å¿µï¼š
1. å¯¼å…¥æ¨¡å— (import)
2. å˜é‡å’Œæ•°æ®ç±»å‹
3. å­—ç¬¦ä¸²æ“ä½œ
4. åˆ—è¡¨ (list) å’Œå¾ªç¯ (for)
5. æ¡ä»¶åˆ¤æ–­ (if/else)
6. å¼‚å¸¸å¤„ç† (try/except)
7. æ–‡ä»¶æ“ä½œ
8. å‡½æ•°å®šä¹‰
9. é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼ˆç±»ã€å¯¹è±¡ã€æ–¹æ³•ï¼‰

æ³¨æ„ï¼šAI ç›¸å…³åŠŸèƒ½å·²ç§»åŠ¨åˆ° Sec2/Lec1ï¼Œå¦‚éœ€ä½¿ç”¨ AI åŠŸèƒ½ï¼Œè¯·ä» Sec2/Lec1 å¯¼å…¥ AIAssistant ç±»ã€‚
"""

# ========== ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥æ¨¡å— ==========
import feedparser      # è§£æ RSS è®¢é˜…æº
from bs4 import BeautifulSoup  # è§£æ HTML
import os              # æ–‡ä»¶ç³»ç»Ÿæ“ä½œ
import requests        # å‘é€ HTTP è¯·æ±‚
import json            # JSON å¤„ç†

# å°è¯•ä» Sec2/Lec1 å¯¼å…¥ AIAssistantï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    import sys
    from pathlib import Path
    # æ·»åŠ  Sec2/Lec1 ç›®å½•åˆ°è·¯å¾„
    # news_oop.py åœ¨ Sec1/Lec3ï¼Œéœ€è¦æ‰¾åˆ° Sec2/Lec1
    current_path = Path(__file__).resolve()
    # ä» Sec1/Lec3 å›åˆ° Courseï¼Œç„¶åè¿›å…¥ Sec2/Lec1
    sec2_lec1_path = current_path.parent.parent.parent / "Sec2" / "Lec1"
    if str(sec2_lec1_path) not in sys.path:
        sys.path.insert(0, str(sec2_lec1_path))
    from ai_assistant import AIAssistant
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False
    AIAssistant = None


# ========== ç¬¬äºŒæ­¥ï¼šå®šä¹‰ NewsCrawler ç±» ==========
class NewsCrawler:
    """æ–°é—»çˆ¬è™«ç±»"""
    
    def __init__(self, rss_url, api_key=None, data_dir="data"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param rss_url: RSS è®¢é˜…æºåœ°å€
        :param api_key: AI API å¯†é’¥ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™å¯ç”¨ AI æ‘˜è¦åŠŸèƒ½ï¼‰
        :param data_dir: æ•°æ®ä¿å­˜ç›®å½•
        """
        self.rss_url = rss_url
        self.data_dir = data_dir
        self.news_count = 0
        
        # å¦‚æœæä¾›äº† API key ä¸” AI åŠŸèƒ½å¯ç”¨ï¼Œåˆ™åˆå§‹åŒ– AI åŠ©æ‰‹
        if api_key and AI_AVAILABLE:
            self.ai_assistant = AIAssistant(api_key)
            self.ai_enabled = True
        else:
            self.ai_assistant = None
            self.ai_enabled = False
            if api_key and not AI_AVAILABLE:
                print("âš ï¸  è­¦å‘Šï¼šAI åŠŸèƒ½ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ AI æ‘˜è¦åŠŸèƒ½")
        
        # åˆ›å»ºä¿å­˜æ–‡ä»¶å¤¹
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
    
    def clean_filename(self, title):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
        filename = title
        for char in [":", "/", "\\", "?", "*", "<", ">", "|", '"']:
            filename = filename.replace(char, "")
        filename = filename.strip()
        if len(filename) > 50:
            filename = filename[:50]
        return filename
    
    def download_article(self, link):
        """
        ä¸‹è½½å¹¶è§£ææ–°é—»æ­£æ–‡
        :param link: æ–°é—»é“¾æ¥
        :return: (HTMLå†…å®¹, çº¯æ–‡æœ¬å†…å®¹)ï¼Œå¤±è´¥è¿”å›("", "")
        """
        try:
            # ä¸‹è½½ç½‘é¡µ
            response = requests.get(link, timeout=10)
            response.encoding = response.apparent_encoding
            html_content = response.text
            
            # è§£æ HTML
            soup = BeautifulSoup(html_content, "html.parser")
            
            # å°è¯•æ‰¾åˆ°æ–‡ç« ä¸»ä½“
            article_div = soup.find("div", class_="article") or soup.find("div", class_="article-content")
            if article_div:
                paragraphs = article_div.find_all("p")
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šçš„ divï¼Œå°±æå–æ‰€æœ‰æ®µè½
                paragraphs = soup.find_all("p")
            
            text_list = []
            for p in paragraphs:
                text = p.get_text(strip=True)
                if len(text) > 10:  # åªä¿ç•™è¾ƒé•¿çš„æ®µè½
                    text_list.append(text)
            
            full_text = "\n\n".join(text_list)
            return html_content, full_text
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}\n")
            return "", ""
    
    def summarize_content(self, title, content):
        """
        ä½¿ç”¨ AI æ€»ç»“æ–°é—»å†…å®¹ï¼ˆå¦‚æœ AI åŠŸèƒ½å¯ç”¨ï¼‰
        :param title: æ–°é—»æ ‡é¢˜
        :param content: æ–°é—»å†…å®¹
        :return: AI ç”Ÿæˆçš„æ‘˜è¦ï¼Œå¦‚æœ AI ä¸å¯ç”¨åˆ™è¿”å›æç¤ºä¿¡æ¯
        """
        if not self.ai_enabled:
            return "AI æ‘˜è¦åŠŸèƒ½æœªå¯ç”¨ï¼ˆéœ€è¦ API key ä¸” AI æ¨¡å—å¯ç”¨ï¼‰"
        
        if not content or len(content) < 50:
            return "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"
        
        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å… token è¿‡å¤š
        content_preview = content[:2000]
        
        prompt = f"""è¯·ç”¨ä¸­æ–‡æ€»ç»“ä»¥ä¸‹æ–°é—»å†…å®¹ï¼Œè¦æ±‚ï¼š
1. ç®€æ´æ˜äº†ï¼Œ3-5å¥è¯
2. çªå‡ºå…³é”®ä¿¡æ¯
3. ä¿æŒå®¢è§‚ä¸­ç«‹

æ–°é—»æ ‡é¢˜ï¼š{title}

æ–°é—»å†…å®¹ï¼š
{content_preview}
"""
        
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆ AI æ‘˜è¦...")
        summary = self.ai_assistant.get_response(prompt)
        return summary if summary else "AI æ‘˜è¦ç”Ÿæˆå¤±è´¥"
    
    def save_article(self, title, link, html_content, text_content):
        """
        ä¿å­˜æ–°é—»åˆ°æ–‡ä»¶ï¼ˆåŒ…æ‹¬ HTML å’Œæ–‡æœ¬ï¼‰
        :param title: æ–°é—»æ ‡é¢˜
        :param link: æ–°é—»é“¾æ¥
        :param html_content: HTML åŸå§‹å†…å®¹
        :param text_content: çº¯æ–‡æœ¬å†…å®¹
        """
        filename_base = self.clean_filename(title)
        
        # 1. ä¿å­˜ HTML æ–‡ä»¶
        if html_content:
            html_filepath = os.path.join(self.data_dir, filename_base + ".html")
            try:
                with open(html_filepath, "w", encoding="utf-8") as f:
                    f.write(html_content)
                print(f"ğŸ’¾ å·²ä¿å­˜ HTML: {filename_base}.html")
            except Exception as e:
                print(f"âŒ ä¿å­˜ HTML å¤±è´¥: {e}")
        
        # 2. ç”Ÿæˆ AI æ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        summary = self.summarize_content(title, text_content) if self.ai_enabled else None
        
        # 3. ä¿å­˜æ–‡æœ¬æ–‡ä»¶ï¼ˆåŒ…å«åŸæ–‡å’Œ AI æ‘˜è¦ï¼Œå¦‚æœæœ‰ï¼‰
        txt_filepath = os.path.join(self.data_dir, filename_base + ".txt")
        try:
            with open(txt_filepath, "w", encoding="utf-8") as f:
                f.write("æ ‡é¢˜: " + title + "\n")
                f.write("é“¾æ¥: " + link + "\n")
                if summary:
                    f.write("\n" + "=" * 50 + "\n")
                    f.write("AI æ‘˜è¦ï¼š\n")
                    f.write(summary + "\n")
                    f.write("\n" + "=" * 50 + "\n\n")
                f.write("åŸæ–‡å†…å®¹ï¼š\n\n")
                f.write(text_content if text_content else "âš ï¸ æ­£æ–‡æŠ“å–å¤±è´¥æˆ–ä¸ºç©º")
            print(f"ğŸ’¾ å·²ä¿å­˜æ–‡æœ¬: {filename_base}.txt\n")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡æœ¬å¤±è´¥: {e}\n")
        
        self.news_count += 1
    
    def run(self, max_news=10):
        """
        è¿è¡Œçˆ¬è™«ä¸»æµç¨‹
        :param max_news: æœ€å¤šçˆ¬å–å¤šå°‘æ¡æ–°é—»
        """
        # è§£æ RSS è®¢é˜…æº
        feed = feedparser.parse(self.rss_url)
        print(f"ğŸ—ï¸ æ‰¾åˆ° {len(feed.entries)} æ¡æ–°é—»\n")
        
        # éå†æ–°é—»å¹¶ä¿å­˜
        for entry in feed.entries[:max_news]:
            title = entry.title
            link = entry.link
            
            print("ğŸ“° æ ‡é¢˜:", title)
            print("ğŸ”— é“¾æ¥:", link)
            
            # ä¸‹è½½æ–°é—» HTML å’Œæ­£æ–‡
            html_content, text_content = self.download_article(link)
            
            # å¦‚æœæ­£æ–‡å¤ªçŸ­ï¼Œæ ‡è®°ä½†ä»ç„¶ä¿å­˜
            if len(text_content) < 30:
                print("âš ï¸  æ­£æ–‡è¾ƒçŸ­æˆ–æŠ“å–ä¸å®Œæ•´")
                text_content = "âš ï¸ æ­£æ–‡æŠ“å–å¤±è´¥æˆ–ä¸ºç©ºï¼Œå¯æ‰“å¼€å¯¹åº” HTML æŸ¥çœ‹"
            else:
                print(f"ğŸ“„ æ­£æ–‡: {len(text_content)} å­—")
            
            # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆåŒ…æ‹¬ HTML å’Œæ–‡æœ¬ï¼Œå¦‚æœå¯ç”¨ AI åˆ™åŒ…å«æ‘˜è¦ï¼‰
            self.save_article(title, link, html_content, text_content)
        
        print(f"âœ… å®Œæˆï¼å…±ä¿å­˜ {self.news_count} æ¡æ–°é—»")


# ========== ç¬¬å››æ­¥ï¼šä½¿ç”¨ç±»åˆ›å»ºå¯¹è±¡å¹¶è¿è¡Œ ==========
if __name__ == "__main__":
    # API å¯†é’¥ï¼ˆå»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
    API_KEY = os.getenv("API_KEY") or "sk-23qfb76qghixbui2"
    
    if not API_KEY:
        raise RuntimeError("API_KEY æœªè®¾ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–åœ¨ä»£ç ä¸­æŒ‡å®š")
    
    # åˆ›å»ºçˆ¬è™«å¯¹è±¡
    rss_url = "http://news.baidu.com/n?cmd=4&class=civilnews&tn=rss"
    crawler = NewsCrawler(rss_url, api_key=API_KEY, data_dir="data")
    
    # è¿è¡Œçˆ¬è™«ï¼ˆé»˜è®¤çˆ¬å– 10 æ¡ï¼‰
    crawler.run(max_news=10)

